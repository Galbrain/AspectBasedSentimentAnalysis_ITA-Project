# -*- coding: utf-8 -*-
import glob
import json
import re

import pandas as pd
import spacy


class Preprocessor:
    """
    Preprocessor class
    """

    def __init__(
        self,
        path,
        lower=True,
        rmnonalphanumeric=True,
        substituespecial=True,
        lemmanize=False,
        rmstopwords=True,
        rmdefault=True,
    ):
        """
        Constructor for Preprocessor class

        Args:
            path (str): path to data as string
            lower (bool, optional): remove capitalization. Defaults to True.
            rmnonalphanumeric (bool, optional): removeNonAlphanumeric. Defaults to True.
            substituespecial (bool, optional): substitue {ä,ö,ü,ß}. Defaults to True.
            lemmanize (bool, optional): find the lemma of all words in text. Defaults to False.
            rmstopwords (bool, optional): rm stopwords from spacey stopword list. Defaults to True.
            rmdefault (bool, optional): rm default phrase from the text. Defaults to True.
        """

        self.path = path
        self.lower = lower
        self.rmnonalphanumeric = rmnonalphanumeric
        self.substituespecial = substituespecial
        self.lemmanize = lemmanize
        self.rmstopwords = rmstopwords
        self.rmdefault = rmdefault
        self.stopwords = {}
        self.substitutedict = {}
        self.nlp = None

    def find_jsons(self) -> list[str]:
        """
        find JSON files from path

        Raises:
            Exception: No files found

        Returns:
            files (list[str]): returns a list with filenames of JSON files
        """

        files = glob.glob(self.path + "*.json")
        if not files:
            raise Exception("No JSON files found!")
        return files

    def extract_text(self, file: str) -> pd.Series:
        """
        extract text from reviews in file

        Args:
            file (str): filename of the file in path

        Returns:
            pd.Series: returns Series with text from reviews
        """

        with open(file, "r") as f:
            json_f = json.load(f)

            return pd.Series(
                [
                    review["text"]
                    for review in json_f["reviews"]
                    if not review["rating"] == {}
                ]
            )

    def extract_rating(self, file: str):
        """
        extract rating from reviews in file

        Args:
            file (str): filename of the file in path

        Returns:
            pd.Series: returns Series with ratings of the reviews
        """

        with open(file, "r") as f:
            json_f = json.load(f)
            return pd.Series([review["rating"] for review in json_f["reviews"]])

    def removeCapitalization(self, series: pd.Series) -> pd.Series:
        """
        remove capitalization from the series

        Args:
            series (pd.Series): Series containing all text from reviews

        Returns:
            series (pd.Series): Series with all lower letters
        """

        series = series.str.lower()
        return series

    def rmNonAlphaNumeric(self, series: pd.Series) -> pd.Series:
        """
        remove all non alpha numerica characters from the text in series

        Args:
            series (pd.Series): Series containing all text from reviews

        Returns:
            series (pd.Series): Series without non alpha numeric text
        """

        return series.str.replace(r"[^\w\s]", "")

    def removeString(self, series: pd.Series, regstring: str):
        """
        remove strings with schema

        Args:
            series (pd.Series): Series containing review text
            regstring (str, optional): string to be removed.

        Returns:
            series (pd.Series): Series with strings Removed
        """

        return series.str.replace(regstring, "")

    def removeDefaultStrings(self, series: pd.Series) -> pd.Series:
        """
        remove default string: Von %USER% (1) :, Ist diese Meinung hilfreich?, INT von INT Lesern fand diese Meinung hilfreich

        Args:
            series (pd.Series): Series containing text

        Returns:
            series (pd.Series): Series without the most common phrased generated by the website
        """

        series = self.removeString(series, "\n")
        series = self.removeString(series, r"[vV]on\s\w+\s+(\(\d+\))?:")
        series = self.removeString(series, r"[iI]st diese [mM]einung hilfreich(\?)?")
        series = self.removeString(series, r"\d+\s\w+\s\d+(\s\w+)+\.(\s\w+)+\?")
        return series

    def substitueSpecial(
        self,
        series: pd.Series,
        dict: dict = {ord("ä"): "ae", ord("ü"): "ue", ord("ö"): "oe", ord("ß"): "ss"},
    ) -> pd.Series:
        """
        substitue the special characters for text and stopwords with their non utf-8 counterparts

        Args:
            series (pd.Series): Series containing the text from reviews
            dict (dict, optional): dictionary of the characters to substitute. Defaults to {ord("ä"): "ae", ord("ü"): "ue", ord("ö"): "oe", ord("ß"): "ss"}.

        Returns:
            series (pd.Series): Series containing text with subsituted special characters
        """

        self.substitutedict = dict
        return series.apply(lambda x: x.translate(self.substitutedict))

    def tokenize(self, series: pd.Series) -> pd.Series:
        """
        tokenize the series

        Args:
            series (pd.Series): Series containing text

        Returns:
            series (pd.Series): Series containing array of tokens
        """

        return series.str.split()

    def loadSpacyModel(
        self,
        model: str = "de_core_news_sm",
        disableList: list[str] = ["tagger", "parser", "ner"],
    ) -> bool:
        """
        load the spacy model with required modes

        Args:
            model (str, optional): name of the mode. Defaults to "de_core_news_sm".
            disableList (list[str], optional): list of things to be disabled. Defaults to ["tagger", "parser", "ner"].
        """

        try:
            self.nlp = spacy.load(model, disable=disableList)
            return True
        except OSError as oe:
            print(oe)
            try:
                spacy.cli.download(model)
            except Exception as e:
                print(e)
                return False
            self.nlp = spacy.load(model, disable=disableList)
            return True

    def loadStopwords(self) -> bool:
        """
        load Stopwords from spacy

        Return:
            bool: whenver the function completed successfully
        """
        if not self.nlp:
            if not self.loadSpacyModel():
                print("Skipping. Unable to load SpacyModel")
                return False

        self.stopwords = self.nlp.Defaults.stop_words
        return True

    def removeStopwords(self, series: pd.Series) -> pd.Series:
        """
        removes the stopwords from tokenized series

        Args:
            series (pd.Series): Series containing tokenized text

        Returns:
            series (pd.Series): tokenized Series without stopwords
        """

        if not self.loadStopwords():
            print("Skipping. Unable to load Stopwords!")
            return series

        if self.substituespecial:
            self.stopwords = {
                word.translate(self.substitutedict) for word in self.stopwords
            }

        return series.apply(
            lambda x: [
                word for word in x if (word not in self.stopwords and word != "")
            ]
        )

    def lemmanizeTokens(self, tokenized_series: pd.Series) -> pd.Series:
        """
        produces a lemmatized version of the tokenized series it was given

        Args:
            tokenized_series (pd.Series): already tokenized series

        Returns:
            lemmanized_series (pd.Series): a lemmanized series
        """
        if not self.nlp:
            if not self.loadSpacyModel():
                print("Skipping. Unable to load Spacy Model")
                return tokenized_series

        lemmanized_series = tokenized_series.apply(
            lambda x: [word.lemma_ for word in self.nlp(" ".join(x))]
        )

        return lemmanized_series

    def prep(self) -> pd.Series:
        """
        prepares the data with the configuations given at initialization

        Returns:
            pd.Series: tokenized Series
        """

        text = pd.Series(dtype=str)

        files = self.find_jsons()

        for file in files:
            text = text.append(self.extract_text(file), ignore_index=True)

        if self.substituespecial:
            text = self.substitueSpecial(text)

        if self.rmnonalphanumeric:
            text = self.rmNonAlphaNumeric(text)

        if self.lower:
            text = self.removeCapitalization(text)

        if self.rmdefault:
            text = self.removeDefaultStrings(text)

        text_tokenized = pd.Series(dtype=str)
        if self.rmstopwords:
            text_tokenized = self.tokenize(text)
            text_tokenized = self.removeStopwords(text_tokenized)
        else:
            text_tokenized = self.tokenize(text)

        if self.lemmanize:
            return self.lemmanizeTokens(text_tokenized)
        else:
            return text_tokenized
